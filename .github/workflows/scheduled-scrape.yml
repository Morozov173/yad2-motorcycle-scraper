name: Run Scraper with UltraViolet Every Saturday

on:
  schedule:
    # Runs at 00:00 UTC every Saturday
    - cron: '0 12 * * 6' # 9 AM IL time
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      # 1. Check out repository
      - name: Check out code
        uses: actions/checkout@v4

      # 2. Install uv
      - name: Install uv
        uses: astral-sh/setup-uv@v5
        
      # 3. Set up Python
      - name: Set-Up Python
        run: uv python install 

      - name: Create .env from GitHub Secrets
        run: |
          echo "PROXY_USERNAME=${{ secrets.PROXY_USERNAME }}" >> .env
          echo "PROXY_PASSWORD=${{ secrets.PROXY_PASSWORD }}" >> .env
          echo "PROXY_SERVER=${{ secrets.PROXY_SERVER }}" >> .env
          echo 'PROXY_LINK=http://${PROXY_USERNAME}:${PROXY_PASSWORD}@${PROXY_SERVER}' >> .env

      # 4. Run the scraper script via uv
      - name: Run scraper
        run: uv run main.py 

      # 5. Commit and push any changes
      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          # Only commit if there are changes
          git diff-index --quiet HEAD || git commit -m "Auto-update from scraper"
          git push
